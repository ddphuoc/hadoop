This guideline is step by step to install Hadoop version 3.3.0
Source: https://phoenixnap.com/kb/install-hadoop-ubuntu

1. Login Ubuntu with user hadoop: su - hdoop

2. Download Hadoop installation: wget https://mirrors.ocf.berkeley.edu/apache/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz

3. Once the download is complete, extract the files to initiate the Hadoop installation (install command tar if it not available using apt): tar xzf hadoop-3.3.0.tar.gz

Single Node Hadoop Deployment (Pseudo-Distributed Mode)
Hadoop excels when deployed in a fully distributed mode on a large cluster of networked servers. However, if you are new to Hadoop and want to explore basic commands or test applications, you can configure Hadoop on a single node. This setup, also called pseudo-distributed mode, allows each Hadoop daemon to run as a single Java process. A Hadoop environment is configured by editing a set of configuration files:
    bashrc
    hadoop-env.sh
    core-site.xml
    hdfs-site.xml
    mapred-site-xml
    yarn-site.xml
    
4. Edit the .bashrc shell configuration file using a text editor of your choice (we will be using nano): nano .bashrc

Define the Hadoop environment variables by adding the following content to the end of the file:
#Hadoop Related Options
export HADOOP_HOME=/home/hdoop/hadoop-3.3.0
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_OPTS"-Djava.library.path=$HADOOP_COMMON_LIB_NATIVE_DIR"

5. It is vital to apply the changes to the current running environment by using the following command: source ~/.bashrc

6. Edit hadoop-env.sh File: nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh

7. Uncomment the $JAVA_HOME variable (i.e., remove the # sign) and add the full path to the JDK installation on your system. If you have installed the same version as presented in the first part of this tutorial (https://github.com/ddphuoc/hadoop/blob/main/environment/installjdk.html), add the following line: export JAVA_HOME=/usr/lib/jvm/java-1.8.0-amazon-corretto

8. Create tmp folder: mkdir /home/hdoop/tmpdata

9. Edit core-site.xml File: nano $HADOOP_HOME/etc/hadoop/core-site.xml
The core-site.xml file defines HDFS and Hadoop core properties. To set up Hadoop in a pseudo-distributed mode, you need to specify the URL for your NameNode, and the temporary directory Hadoop uses for the map and reduce process.
Add the following configuration to override the default values for the temporary directory and add your HDFS URL to replace the default local file system setting:
<configuration>
<property>
  <name>hadoop.tmp.dir</name>
  <value>/home/hdoop/tmpdata</value>
</property>
<property>
  <name>fs.default.name</name>
  <value>hdfs://127.0.0.1:9000</value>
</property>
</configuration>

10. Create folder for namenode and datanode
mkdir -p /home/hdoop/dfsdata/namenode
mkdir -p /home/hdoop/dfsdata/datanode

11. Edit hdfs-site.xml File: nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
The properties in the hdfs-site.xml file govern the location for storing node metadata, fsimage file, and edit log file. Configure the file by defining the NameNode and DataNode storage directories. Additionally, the default dfs.replication value of 3 needs to be changed to 1 to match the single node setup.
Add the following configuration to the file and, if needed, adjust the NameNode and DataNode directories to your custom locations:
<configuration>
<property>
  <name>dfs.data.dir</name>
  <value>/home/hdoop/dfsdata/namenode</value>
</property>
<property>
  <name>dfs.data.dir</name>
  <value>/home/hdoop/dfsdata/datanode</value>
</property>
<property>
  <name>dfs.replication</name>
  <value>1</value>
</property>
</configuration>

12. Edit mapred-site.xml File: nano $HADOOP_HOME/etc/hadoop/mapred-site.xml
Use the following command to access the mapred-site.xml file and define MapReduce values
Add the following configuration to change the default MapReduce framework name value to yarn:
<configuration> 
<property> 
  <name>mapreduce.framework.name</name> 
  <value>yarn</value> 
</property> 
</configuration>

13. Edit yarn-site.xml File: nano $HADOOP_HOME/etc/hadoop/yarn-site.xml
The yarn-site.xml file is used to define settings relevant to YARN. It contains configurations for the Node Manager, Resource Manager, Containers, and Application Master.
Append the following configuration to the file:
<configuration>
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>
<property>
  <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
  <value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>127.0.0.1</value>
</property>
<property>
  <name>yarn.acl.enable</name>
  <value>0</value>
</property>
<property>
  <name>yarn.nodemanager.env-whitelist</name>   
  <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PERPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
</property>
</configuration>

14. Format HDFS NameNode: hdfs namenode -format
It is important to format the NameNode before starting Hadoop services for the first time

15. Change folder to hadoop sbin: cd hadoop-3.3.0/sbin

16. Start Hadoop Cluster: start-dfs.sh
Navigate to the hadoop-3.3.0/sbin directory and execute the following commands to start the NameNode and DataNode

17. Once the namenode, datanodes, and secondary namenode are up and running, start the YARN resource and nodemanagers by typing: start-yarn.sh
